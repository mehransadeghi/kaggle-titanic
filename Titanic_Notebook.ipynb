{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85572a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c3324d",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63641781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data():\n",
    "    \"\"\" Age preprocessing: Adding Age_is_missing col and filling the empty age with an estimate based on Sex and PClass \"\"\"\n",
    "    train_df['Age_Missing'] = train_df['Age'].isna().astype(int)\n",
    "    test_df['Age_Missing'] = test_df['Age'].isna().astype(int)\n",
    "    \n",
    "    global_age = train_df['Age'].median()\n",
    "    group_age = train_df.groupby(['Sex','Pclass'])['Age'].median()\n",
    "    def estimate_age(row):\n",
    "        if pd.isna(row['Age']):\n",
    "            age = group_age.get((row['Sex'], row['Pclass']), np.nan)\n",
    "            return age if not np.isnan(age) else global_age\n",
    "        return row['Age']\n",
    "    \n",
    "    train_df['Age'] = train_df.apply(estimate_age, axis=1)\n",
    "    test_df['Age'] = test_df.apply(estimate_age, axis=1)\n",
    "    \n",
    "    \"\"\" Adding a Family_size col that reflects sum of sibsp and parch \"\"\"\n",
    "    \n",
    "    train_df['Family_Size'] = train_df['Parch'] + train_df['SibSp'] + 1\n",
    "    test_df['Family_Size'] = test_df['Parch'] + test_df['SibSp'] + 1\n",
    "    \n",
    "    \"\"\" Fare preprocessing: Filling the empty fare with an estimate based on Sex and PClass \"\"\"\n",
    "    global_fare = train_df['Fare'].median()\n",
    "    group_fare = train_df.groupby(['Sex', 'Pclass'])['Fare'].median()\n",
    "    \n",
    "    def estimate_fare(row):\n",
    "        if pd.isna(row['Fare']):\n",
    "            fare = group_fare.get((row['Sex'], row['Pclass']), np.nan)\n",
    "            return fare if not pd.isna(fare) else global_fare\n",
    "        return row['Fare']\n",
    "    train_df['Fare'] = train_df.apply(estimate_fare, axis=1)\n",
    "    test_df['Fare'] = test_df.apply(estimate_fare, axis=1)\n",
    "    \n",
    "    \"\"\" Change cabin to be the first letter of cabin and also fill with U if it is empty \"\"\"\n",
    "    train_df['Cabin'] = train_df['Cabin'].str[0].fillna('U')\n",
    "    test_df['Cabin'] = test_df['Cabin'].str[0].fillna('U')\n",
    "    \n",
    "    \"\"\" If Embarked is empty, use 'U' \"\"\"\n",
    "    train_df['Embarked'] = train_df['Embarked'].fillna('U')\n",
    "    test_df['Embarked'] = test_df['Embarked'].fillna('U')\n",
    "    \n",
    "    \"\"\" Convert Sex to numerical col (female=1, male=0)\"\"\"\n",
    "    train_df['Sex'] = (train_df['Sex']=='female').astype(int)\n",
    "    test_df['Sex'] = (test_df['Sex']=='female').astype(int)\n",
    "    \n",
    "    \n",
    "    \"\"\" Convert other cat cols to numerical cols \"\"\"\n",
    "    cat_cols = ['Cabin', 'Embarked']\n",
    "    x_train_cat = pd.get_dummies(train_df[cat_cols], prefix=cat_cols, dummy_na=False)\n",
    "    x_test_cat = pd.get_dummies(test_df[cat_cols], prefix=cat_cols, dummy_na=False)\n",
    "    x_test_cat = x_test_cat.reindex(columns=x_train_cat.columns, fill_value=0)\n",
    "    \n",
    "    \"\"\" Standardize some numerical cols \"\"\"\n",
    "    cols_to_standardize = ['Age', 'Fare', 'Family_Size']\n",
    "    mu = train_df[cols_to_standardize].mean()\n",
    "    std = train_df[cols_to_standardize].std(ddof=0)\n",
    "    std = std.replace(0, 1.0)\n",
    "    train_df[cols_to_standardize] = (train_df[cols_to_standardize]-mu)/std\n",
    "    test_df[cols_to_standardize] = (test_df[cols_to_standardize]-mu)/std\n",
    "    \n",
    "    \"\"\" Select numerical cols that we want to use in logistic regression \"\"\"\n",
    "    num_cols = ['Sex', 'Age', 'Age_Missing', 'Family_Size', 'Fare']\n",
    "    x_train_num = train_df[num_cols].astype(float)\n",
    "    x_test_num = test_df[num_cols].astype(float)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    x_train = pd.concat([x_train_num, x_train_cat], axis=1)\n",
    "    x_test = pd.concat([x_test_num, x_test_cat], axis=1)\n",
    "    \n",
    "    return x_train, x_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e916081f",
   "metadata": {},
   "source": [
    "## Performing data preprocessing and converting to NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3441198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df, x_test_df = preprocess_data()\n",
    "\n",
    "x_train = x_train_df.to_numpy(dtype=np.float64, copy=False)\n",
    "x_test = x_test_df.to_numpy(dtype=np.float64, copy=False)\n",
    "y_train = train_df['Survived'].to_numpy(dtype=np.float64, copy=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43d913e",
   "metadata": {},
   "source": [
    "## Logistic Regression From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b8427ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def logistic_cost(X, y, w, b, lambda_):\n",
    "    z = X @ w + b\n",
    "    return (np.logaddexp(0, z) - y*z).mean()\n",
    "\n",
    "def regularized_logistic_cost(X, y, w, b, lambda_):\n",
    "    z = X @ w + b\n",
    "    return (np.logaddexp(0, z) - y*z).mean() + (lambda_/(2*m))*np.dot(w, w)\n",
    "\n",
    "def compute_gradient(X, y, w, b, lambda_):\n",
    "    m, n = X.shape\n",
    "    z = X @ w + b\n",
    "    f_x = sigmoid(z)\n",
    "    err = f_x - y\n",
    "    d_j_b = err.mean()\n",
    "    d_j_w = (X.T @ err) /m \n",
    "    return d_j_w, d_j_b\n",
    "\n",
    "def compute_regularized_gradient(X, y, w, b, lambda_):\n",
    "    m, n = X.shape\n",
    "    z = X @ w + b\n",
    "    f_x = sigmoid(z)\n",
    "    err = f_x - y\n",
    "    d_j_b = err.mean()\n",
    "    d_j_w = ((X.T @ err) + lambda_*w) /m\n",
    "    return d_j_w, d_j_b\n",
    "\n",
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_):\n",
    "    w, b = w_in, b_in\n",
    "    cost_history =[]\n",
    "    for i in range(num_iters):\n",
    "        d_j_w, d_j_b = gradient_function(X, y, w, b, lambda_)\n",
    "        w  = w - alpha * d_j_w\n",
    "        b = b - alpha * d_j_b\n",
    "        cost = cost_function(X, y, w, b, lambda_)\n",
    "        if i % 10000 == 0: \n",
    "            print(f\"cost in iteration {i} = {cost}\")\n",
    "            cost_history.append(cost)\n",
    "    \n",
    "    return w, b, cost_history\n",
    "\n",
    "\n",
    "\n",
    "def predict(X, w, b):\n",
    "    z = X @ w + b\n",
    "    f_x = sigmoid(z)\n",
    "    return [int(x>=0.5) for x in f_x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "83dbc1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost in iteration 0 = 0.6980479313369613\n",
      "cost in iteration 10000 = 0.5199743077136791\n",
      "cost in iteration 20000 = 0.4828057758196072\n",
      "cost in iteration 30000 = 0.4672616025323831\n",
      "cost in iteration 40000 = 0.4595522879004116\n",
      "cost in iteration 50000 = 0.45533215184578174\n",
      "cost in iteration 60000 = 0.45284983170867016\n",
      "cost in iteration 70000 = 0.4513006929463314\n",
      "cost in iteration 80000 = 0.4502827331712428\n",
      "cost in iteration 90000 = 0.44958232272471255\n",
      "Train Accuracy: 80.134680\n"
     ]
    }
   ],
   "source": [
    "# Initialize parameters for non regularized logistic regression\n",
    "m, n = x_train.shape\n",
    "# w = np.zeros(n)\n",
    "# b = 0\n",
    "np.random.seed(1)\n",
    "w = 0.01 * (np.random.rand(n) - 0.5)\n",
    "b = -1\n",
    "\n",
    "alpha = 0.001\n",
    "\n",
    "\n",
    "\"\"\" Update w and b using gradient descent \"\"\"\n",
    "w, b, costs = gradient_descent(x_train, y_train, w, b, logistic_cost, compute_gradient, alpha, 100000, lambda_)\n",
    "\n",
    "\n",
    "predicted_values = predict(x_train, w, b)\n",
    "print('Train Accuracy: %f'%(np.mean(predicted_values == y_train) * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b4436420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost in iteration 0 = 0.6980478604348902\n",
      "cost in iteration 10000 = 0.5187560507134266\n",
      "cost in iteration 20000 = 0.4803163592993963\n",
      "cost in iteration 30000 = 0.46380899375407647\n",
      "cost in iteration 40000 = 0.45538729597507\n",
      "cost in iteration 50000 = 0.4506374179618552\n",
      "cost in iteration 60000 = 0.4477558023924199\n",
      "cost in iteration 70000 = 0.44590049750694194\n",
      "cost in iteration 80000 = 0.44464329096253047\n",
      "cost in iteration 90000 = 0.4437521809576839\n",
      "Train Accuracy: 80.134680\n"
     ]
    }
   ],
   "source": [
    "# Initialize parameters for regularized logistic regression\n",
    "m, n = x_train.shape\n",
    "# w = np.zeros(n)\n",
    "# b = 0\n",
    "np.random.seed(1)\n",
    "w = 0.01 * (np.random.rand(n) - 0.5)\n",
    "b = -1\n",
    "\n",
    "alpha = 0.001\n",
    "lambda_ = -1\n",
    "\n",
    "\n",
    "\"\"\" Update w and b using gradient descent \"\"\"\n",
    "w, b, costs = gradient_descent(x_train, y_train, w, b, regularized_logistic_cost, compute_regularized_gradient, alpha, 100000, lambda_)\n",
    "\n",
    "predicted_values = predict(x_train, w, b)\n",
    "print('Train Accuracy: %f'%(np.mean(predicted_values == y_train) * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90a8831",
   "metadata": {},
   "source": [
    "## Predicting for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8fab7b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 0, 1, 0, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "predicted_test_values = predict(x_test, w, b)\n",
    "\n",
    "print(predicted_test_values[:10])\n",
    "test_df['PassengerId']\n",
    "\n",
    "output_file = pd.DataFrame({\n",
    "    'PassengerId': test_df['PassengerId'].astype(int).values,\n",
    "    'Survived': predicted_test_values\n",
    "})\n",
    "output_file.to_csv('titanic_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6eb69f",
   "metadata": {},
   "source": [
    "## SKLearn baseline + tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f1eb622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def fit_logreg(X, y, c=1.0, class_weight=None, max_iter=10000, random_state=20):\n",
    "    clf = LogisticRegression(\n",
    "        C=c, penalty=\"l2\", solver = \"lbfgs\", class_weight=class_weight, \n",
    "        max_iter=max_iter, random_state=random_state)\n",
    "    clf.fit(X, y)\n",
    "    return clf\n",
    "\n",
    "\n",
    "def train_accuracy(clf, X, y, threshold=0.5):\n",
    "    p = clf.predict_proba(X)[:, 1]\n",
    "    y_hat = (p>= threshold).astype(int)\n",
    "    return accuracy_score(y, y_hat)\n",
    "\n",
    "\n",
    "def predict_labels(clf, X, threshold=0.5):\n",
    "    p = clf.predict_proba(X)[:, 1]\n",
    "    return (p>=threshold).astype(int)    \n",
    "\n",
    "def write_submission(lables, path=\"sklearn-output.csv\"):\n",
    "    output_file = pd.DataFrame({\n",
    "        'PassengerId': test_df['PassengerId'].astype(int).values,\n",
    "        'Survived': lables\n",
    "    })\n",
    "    output_file.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a14bdc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8047138047138047\n"
     ]
    }
   ],
   "source": [
    "clf = fit_logreg(x_train, y_train, c=0.45, max_iter=100000)\n",
    "acc = train_accuracy(clf, x_train, y_train)\n",
    "print(f'Train Accuracy: {acc}')\n",
    "lables = predict_labels(clf, x_test)\n",
    "write_submission(lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a5739e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python KaggleVenv",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
